### Apache Spark

我们将使用的工具之一是Apache Spark。 Spark是集群计算的开源工具集。 虽然我们不会使用集群，但是Spark的典型用法是一组更大的机器或集群，它们并行运行以分析大数据集。 一个

安装指南可在https://www.dataquest.io/blog/pyspark-installation-guide下载。 具体来说，您需要将两个设置添加到您的bash配置文件：SPARK_HOME和PYSPARK_SUBMIT_ARGS。 SPARK_HOME是安装软件的目录。 PYSPARK_SUBMIT_ARGS设置在本地集群中使用的核心数量。
